# app.py
import streamlit as st
import requests
import pandas as pd
import os
from pathlib import Path

st.title("üö≤ TfL BikePoint Live Dashboard")

# Get BikePoint data
url = "https://api.tfl.gov.uk/BikePoint"
response = requests.get(url)
data = response.json()

stations = []
for station in data:
    name = station["commonName"]
    bikes = next((p["value"] for p in station["additionalProperties"] if p["key"] == "NbBikes"), None)
    empty_docks = next((p["value"] for p in station["additionalProperties"] if p["key"] == "NbEmptyDocks"), None)
    total_docks = int(bikes) + int(empty_docks)
    occupancy_rate = round((int(bikes) / total_docks * 100) if total_docks > 0 else 0, 1)
    
    stations.append({
        "Station": name,
        "Bikes": int(bikes),
        "Empty Docks": int(empty_docks),
        "Total Docks": total_docks,
        "Occupancy Rate": occupancy_rate
    })

df = pd.DataFrame(stations)

# Quick stats
col1, col2, col3 = st.columns(3)
with col1:
    st.metric("Total Stations", len(df))
with col2:
    st.metric("Total Bikes Available", df["Bikes"].sum())
with col3:
    st.metric("Average Occupancy", f"{df['Occupancy Rate'].mean():.1f}%")

# Simple bar chart showing top 15 stations by bikes available
st.subheader("Top 15 Stations by Available Bikes")
top_stations = df.nlargest(15, 'Bikes')[['Station', 'Bikes']].set_index('Station')
st.bar_chart(top_stations)

# Data table
st.subheader("All Stations")
st.dataframe(df)

# Optional: filter by station name
station_filter = st.text_input("Filter by station name")
if station_filter:
    filtered_df = df[df["Station"].str.contains(station_filter, case=False)]
    st.subheader("Filtered Results")
    st.dataframe(filtered_df)

# Overview
st.header("Overview")
st.write(
    "This project builds a data pipeline on Google Cloud Platform (GCP) to collect, "
    "process, and store real-time bike availability data in London using the TfL BikePoint API. "
    "The architecture leverages key GCP services such as Cloud Scheduler, Pub/Sub, Cloud Functions, "
    "Cloud SQL, and Compute Engine to automate data ingestion, transformation, and visualization "
    "via this Streamlit web app."
)

# Architecture
st.header("Architecture")

#st.image("diagram.png", caption="Architecture Diagram")


st.write("üîç **Streamlit Cloud Debug:**")
st.write("Current working directory:", os.getcwd())
st.write("Python file location:", __file__)

# Check current directory contents
st.write("Files in current directory:")
for item in sorted(os.listdir(".")):
    if os.path.isfile(item):
        size = os.path.getsize(item)
        st.write(f"üìÑ {item} ({size} bytes)")
    else:
        st.write(f"üìÅ {item}/")

# Specifically check for the image
image_path = Path("Diagram.png")
if image_path.exists():
    st.success("‚úÖ Diagram.png found!")
    st.write(f"File size: {image_path.stat().st_size} bytes")
    st.write(f"Absolute path: {image_path.absolute()}")
else:
    st.error("‚ùå Diagram.png not found in current directory")
    
    # Check if it exists elsewhere
    for root, dirs, files in os.walk("."):
        for file in files:
            if file == "Diagram.png":
                found_path = os.path.join(root, file)
                st.write(f"üîç Found Diagram.png at: {found_path}")

st.subheader("1. Data Source")
st.write("Bike availability and station data is fetched from the Transport for London (TfL) BikePoint API.")

st.subheader("2. Data Ingestion")
st.write(
    "‚Ä¢ A Cloud Scheduler job triggers every hour (CRON: 0 * * * *).\n"
    "‚Ä¢ This scheduler publishes a message to a Pub/Sub topic (bike_ingestion).\n"
    "‚Ä¢ A Cloud Function (ingest_bike_data) is triggered by this topic and fetches data from the API."
)

st.subheader("3. Data Processing")
st.write(
    "‚Ä¢ A Pub/Sub message is called by another Cloud Function to retrieve the fetched data.\n"
    "‚Ä¢ The Cloud Function processes the data, extracts relevant fields, and prepares it for storage."
)

st.subheader("4. Data Storage")
st.write(
    "‚Ä¢ Processed data is inserted into a Cloud SQL (PostgreSQL) database.\n"
    "‚Ä¢ Each record includes station-level info with a timestamp for tracking historical availability."
)

st.subheader("5. Data Visualization")
st.write(
    "‚Ä¢ This Streamlit web app queries the Cloud SQL database, loads the data into pandas DataFrames, and presents visual summaries such as:\n"
    "  - Total available bikes and docks\n"
    "  - Station-level details\n"
    "  - Time-based trends"
)

# Tech Stack
st.header("Tech Stack")

st.subheader("Google Cloud Platform (GCP)")
st.write(
    "- Cloud Scheduler (for timed execution)\n"
    "- Pub/Sub (for messaging)\n"
    "- Cloud Functions (to fetch and process data)\n"
    "- Cloud SQL (PostgreSQL database for storage)\n"
    "- Compute Engine (hosts the Streamlit app)"
)

st.subheader("External Services & Tools")
st.write(
    "- TfL BikePoint API (real-time data source)\n"
    "- Streamlit (Python-based visualization and deployment framework)\n"
    "- Terraform (Infrastructure-as-Code)"
)
